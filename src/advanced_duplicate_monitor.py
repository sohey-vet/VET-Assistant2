#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VET-Assistant2 é«˜åº¦é‡è¤‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
æ•°ã‹æœˆã®éå»æŠ•ç¨¿ã¨ã®å³é‡ãªå†…å®¹é‡è¤‡ãƒã‚§ãƒƒã‚¯
"""

import sqlite3
import hashlib
import re
import json
import os
import difflib
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict, Tuple, Optional

class AdvancedDuplicateMonitor:
    def __init__(self, db_path: str = "vet_assistant2_posts.db"):
        self.db_path = db_path
        self.similarity_threshold = 0.65  # 65%ä»¥ä¸Šã®é¡ä¼¼åº¦ã§é‡è¤‡ã¨åˆ¤å®š
        self.init_database()
        self.load_existing_tweets()
    
    def init_database(self):
        """æŠ•ç¨¿å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŠ•ç¨¿å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS post_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content TEXT NOT NULL,
                content_hash TEXT NOT NULL,
                normalized_content TEXT NOT NULL,
                post_type TEXT,
                animal_type TEXT,
                topic TEXT,
                keywords TEXT,
                main_points TEXT,
                char_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                source TEXT DEFAULT 'generated'
            )
        ''')
        
        # é‡è¤‡æ¤œå‡ºå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS duplicate_detections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                attempted_content TEXT NOT NULL,
                similar_post_id INTEGER,
                similarity_score REAL,
                detection_reason TEXT,
                detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (similar_post_id) REFERENCES post_history (id)
            )
        ''')
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_hash ON post_history(content_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_animal_type ON post_history(animal_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON post_history(topic)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON post_history(created_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_normalized_content ON post_history(normalized_content)')
        
        conn.commit()
        conn.close()
    
    def load_existing_tweets(self):
        """æ—¢å­˜ã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        tweets_file = r"C:\Users\souhe\Desktop\Xéå»æŠ•ç¨¿\data\tweets.js"
        
        if os.path.exists(tweets_file):
            try:
                with open(tweets_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                json_match = re.search(r'window\.YTD\.tweets\.part0\s*=\s*(\[.*\]);?', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    tweets_data = json.loads(json_str)
                    
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()
                    
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    cursor.execute('SELECT COUNT(*) FROM post_history WHERE source = "archive"')
                    if cursor.fetchone()[0] == 0:
                        # éå»æŠ•ç¨¿ã‚’ä¿å­˜
                        for tweet_obj in tweets_data:
                            if 'tweet' in tweet_obj and 'full_text' in tweet_obj['tweet']:
                                full_text = tweet_obj['tweet']['full_text']
                                
                                # çŒ«ã¾ãŸã¯çŠ¬ã®æŠ•ç¨¿ã®ã¿
                                if '#çŒ«ã®ã‚ã‚Œã“ã‚Œ' in full_text or '#ç£åŒ»ãŒæ•™ãˆã‚‹çŠ¬ã®ã¯ãªã—' in full_text:
                                    self.save_historical_post(full_text, cursor)
                        
                        conn.commit()
                    
                    conn.close()
                    
            except Exception as e:
                print(f"âš ï¸ æ—¢å­˜ãƒ„ã‚¤ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_historical_post(self, content: str, cursor):
        """éå»æŠ•ç¨¿ã‚’å±¥æ­´ã«ä¿å­˜"""
        content_hash = self.calculate_content_hash(content)
        normalized_content = self.normalize_content(content)
        keywords = json.dumps(self.extract_keywords(content), ensure_ascii=False)
        main_points = json.dumps(self.extract_main_points(content), ensure_ascii=False)
        
        animal_type = "cat" if '#çŒ«ã®ã‚ã‚Œã“ã‚Œ' in content else "dog"
        topic = self.extract_topic(content)
        
        cursor.execute('''
            INSERT OR IGNORE INTO post_history 
            (content, content_hash, normalized_content, animal_type, topic, 
             keywords, main_points, char_count, source)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (content, content_hash, normalized_content, animal_type, topic,
              keywords, main_points, len(content), 'archive'))
    
    def normalize_content(self, content: str) -> str:
        """æŠ•ç¨¿å†…å®¹ã‚’æ­£è¦åŒ–"""
        # æ”¹è¡Œã€ç©ºç™½ã€çµµæ–‡å­—ã€è¨˜å·ã‚’é™¤å»
        normalized = re.sub(r'[\n\r\s]', '', content)
        normalized = re.sub(r'[^\w\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', '', normalized)
        normalized = re.sub(r'#\w+', '', normalized)
        return normalized.lower()
    
    def extract_keywords(self, content: str) -> List[str]:
        """æŠ•ç¨¿å†…å®¹ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # ç–¾æ‚£ãƒ»ç—‡çŠ¶ãƒ‘ã‚¿ãƒ¼ãƒ³
        disease_patterns = [
            r'(è…è‡“ç—…|è…ä¸å…¨|å¿ƒè‡“ç—…|ç³–å°¿ç—…|ç”²çŠ¶è…º|è‚è‡“|è†€èƒ±|å°¿è·¯|çµçŸ³|æ„ŸæŸ“ç—‡|ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼)',
            r'(ç™½å†…éšœ|ç·‘å†…éšœ|çµè†œç‚|çš®è†šç‚|å¤–è€³ç‚|æ­¯å‘¨ç—…|å£å†…ç‚|é–¢ç¯€ç‚)',
            r'(å˜”å|ä¸‹ç—¢|ä¾¿ç§˜|ç™ºç†±|é£Ÿæ¬²ä¸æŒ¯|ä½“é‡æ¸›å°‘|å‘¼å¸å›°é›£|å¤šé£²å¤šå°¿)',
            r'(è¡€å°¿|ã‚ˆã ã‚Œ|å£è‡­|æ­©è¡Œç•°å¸¸|éœ‡ãˆ|ç—™æ”£|æ„è­˜éšœå®³)'
        ]
        
        # çŒ«ç¨®ãƒ»çŠ¬ç¨®ãƒ‘ã‚¿ãƒ¼ãƒ³
        breed_patterns = [
            r'(ã‚¢ãƒ¡ãƒªã‚«ãƒ³ã‚·ãƒ§ãƒ¼ãƒˆãƒ˜ã‚¢|ãƒšãƒ«ã‚·ãƒ£|ãƒ­ã‚·ã‚¢ãƒ³ãƒ–ãƒ«ãƒ¼|ã‚¹ã‚³ãƒ†ã‚£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰|ãƒãƒ³ãƒã‚«ãƒ³)',
            r'(ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ¼ãƒ³|ãƒ©ã‚°ãƒ‰ãƒ¼ãƒ«|ãƒ™ãƒ³ã‚¬ãƒ«|ã‚¢ãƒ“ã‚·ãƒ‹ã‚¢ãƒ³|ãƒ–ãƒªãƒ†ã‚£ãƒƒã‚·ãƒ¥)',
            r'(ãƒˆã‚¤ãƒ—ãƒ¼ãƒ‰ãƒ«|ãƒãƒ¯ãƒ¯|ãƒ€ãƒƒã‚¯ã‚¹ãƒ•ãƒ³ãƒˆ|ãƒãƒ¡ãƒ©ãƒ‹ã‚¢ãƒ³|ã‚·ãƒ¼ã‚ºãƒ¼)',
            r'(ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼|ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«|æŸ´çŠ¬|ãƒ•ãƒ¬ãƒ³ãƒãƒ–ãƒ«ãƒ‰ãƒƒã‚°)'
        ]
        
        # åŒ»ç™‚ãƒ»ã‚±ã‚¢ç”¨èª
        medical_patterns = [
            r'(è¨ºæ–­|æ²»ç™‚|æ‰‹è¡“|è–¬|ãƒ¯ã‚¯ãƒãƒ³|æ¤œæŸ»|è¡€æ¶²æ¤œæŸ»|ãƒ¬ãƒ³ãƒˆã‚²ãƒ³|ã‚¨ã‚³ãƒ¼|MRI)',
            r'(ç—‡çŠ¶|äºˆé˜²|ã‚±ã‚¢|ç®¡ç†|è¦³å¯Ÿ|å¯¾å‡¦æ³•|å¿œæ€¥å‡¦ç½®|å¥åº·è¨ºæ–­)',
            r'(ãƒ•ãƒ¼ãƒ‰|é£Ÿäº‹|çµ¦é¤Œ|æ°´åˆ†|æ „é¤Š|ã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆ|ãŠã‚„ã¤)',
            r'(ãƒˆã‚¤ãƒ¬|æ’å°¿|æ’ä¾¿|ã‚°ãƒ«ãƒ¼ãƒŸãƒ³ã‚°|ãƒ–ãƒ©ãƒƒã‚·ãƒ³ã‚°|çˆªåˆ‡ã‚Š)'
        ]
        
        all_patterns = disease_patterns + breed_patterns + medical_patterns
        
        for pattern in all_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend(matches)
        
        return list(set(keywords))
    
    def extract_main_points(self, content: str) -> List[str]:
        """ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"""
        points = []
        
        # ç®‡æ¡æ›¸ããƒã‚¤ãƒ³ãƒˆ
        bullet_patterns = [
            r'âœ…\s*([^\n]+)',
            r'ğŸ’¡\s*([^\n]+)',
            r'ğŸ¾\s*([^\n]+)',
            r'âš ï¸\s*([^\n]+)',
            r'ğŸ¥\s*([^\n]+)',
            r'â‘ \s*([^\n]+)',
            r'â‘¡\s*([^\n]+)',
            r'â‘¢\s*([^\n]+)',
            r'â‘£\s*([^\n]+)'
        ]
        
        for pattern in bullet_patterns:
            matches = re.findall(pattern, content)
            points.extend(matches)
        
        # é‡è¦æ–‡ã®æŠ½å‡º
        important_patterns = [
            r'ã€([^ã€‘]+)ã€‘',
            r'æ³¨æ„[^\n]*?[ã€‚ï¼ï¼Ÿ]',
            r'é‡è¦[^\n]*?[ã€‚ï¼ï¼Ÿ]',
            r'å¿…è¦[^\n]*?[ã€‚ï¼ï¼Ÿ]',
            r'ç—‡çŠ¶[^\n]*?[ã€‚ï¼ï¼Ÿ]'
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, content)
            points.extend(matches)
        
        return [point.strip() for point in points if point.strip()]
    
    def extract_topic(self, content: str) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
        # ã€ã€‘å†…ã®ãƒˆãƒ”ãƒƒã‚¯
        topic_match = re.search(r'ã€([^ã€‘]+)ã€‘', content)
        if topic_match:
            return topic_match.group(1)
        
        # ç–¾æ‚£åã®æŠ½å‡º
        diseases = [
            'è…è‡“ç—…', 'è…ä¸å…¨', 'å¿ƒè‡“ç—…', 'ç³–å°¿ç—…', 'ç”²çŠ¶è…º', 'æ­¯å‘¨ç—…', 'é–¢ç¯€ç‚',
            'å°¿è·¯çµçŸ³', 'çš®è†šç‚', 'å¤–è€³ç‚', 'ç™½å†…éšœ', 'ç·‘å†…éšœ'
        ]
        
        for disease in diseases:
            if disease in content:
                return disease
        
        # çŒ«ç¨®ãƒ»çŠ¬ç¨®ã®æŠ½å‡º
        breeds = [
            'ã‚¢ãƒ¡ãƒªã‚«ãƒ³ã‚·ãƒ§ãƒ¼ãƒˆãƒ˜ã‚¢', 'ãƒšãƒ«ã‚·ãƒ£', 'ãƒãƒ³ãƒã‚«ãƒ³', 'ã‚¹ã‚³ãƒ†ã‚£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰',
            'ãƒˆã‚¤ãƒ—ãƒ¼ãƒ‰ãƒ«', 'ãƒãƒ¯ãƒ¯', 'ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼', 'æŸ´çŠ¬'
        ]
        
        for breed in breeds:
            if breed in content:
                return breed
        
        return "ä¸€èˆ¬"
    
    def calculate_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        normalized = self.normalize_content(content)
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def calculate_similarity(self, content1: str, content2: str) -> float:
        """2ã¤ã®æŠ•ç¨¿ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        norm1 = self.normalize_content(content1)
        norm2 = self.normalize_content(content2)
        
        if norm1 == norm2:
            return 1.0
        
        # æ–‡å­—åˆ—ã®é¡ä¼¼åº¦
        text_similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é¡ä¼¼åº¦
        keywords1 = set(self.extract_keywords(content1))
        keywords2 = set(self.extract_keywords(content2))
        
        if keywords1 or keywords2:
            keyword_similarity = len(keywords1 & keywords2) / len(keywords1 | keywords2)
        else:
            keyword_similarity = 0.0
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã®é¡ä¼¼åº¦
        points1 = set(self.extract_main_points(content1))
        points2 = set(self.extract_main_points(content2))
        
        if points1 or points2:
            points_similarity = len(points1 & points2) / len(points1 | points2)
        else:
            points_similarity = 0.0
        
        # é‡ã¿ä»˜ãå¹³å‡
        final_similarity = (
            text_similarity * 0.4 +
            keyword_similarity * 0.4 +
            points_similarity * 0.2
        )
        
        return final_similarity
    
    def check_duplicate_comprehensive(self, content: str, animal_type: str = None, 
                                    topic: str = None, months_back: int = 6) -> Tuple[bool, List[Dict]]:
        """åŒ…æ‹¬çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        content_hash = self.calculate_content_hash(content)
        cursor.execute('SELECT * FROM post_history WHERE content_hash = ?', (content_hash,))
        exact_match = cursor.fetchone()
        
        if exact_match:
            duplicate_info = {
                'type': 'exact_match',
                'similarity': 1.0,
                'content': exact_match[1],
                'topic': exact_match[6],
                'created_at': exact_match[10],
                'source': exact_match[11]
            }
            
            # é‡è¤‡æ¤œå‡ºè¨˜éŒ²
            cursor.execute('''
                INSERT INTO duplicate_detections 
                (attempted_content, similar_post_id, similarity_score, detection_reason)
                VALUES (?, ?, ?, ?)
            ''', (content, exact_match[0], 1.0, 'exact_match'))
            
            conn.commit()
            conn.close()
            return True, [duplicate_info]
        
        # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆéå»æ•°ã‹æœˆï¼‰
        cutoff_date = datetime.now() - timedelta(days=months_back * 30)
        
        # å‹•ç‰©ç¨®ã¨ãƒˆãƒ”ãƒƒã‚¯ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        where_conditions = ["created_at > ?"]
        params = [cutoff_date.strftime('%Y-%m-%d %H:%M:%S')]
        
        if animal_type:
            where_conditions.append("animal_type = ?")
            params.append(animal_type)
        
        if topic:
            where_conditions.append("topic = ?")
            params.append(topic)
        
        query = f'''
            SELECT * FROM post_history 
            WHERE {' AND '.join(where_conditions)}
            ORDER BY created_at DESC
            LIMIT 200
        '''
        
        cursor.execute(query, params)
        candidate_posts = cursor.fetchall()
        
        duplicates = []
        for post in candidate_posts:
            similarity = self.calculate_similarity(content, post[1])
            
            if similarity >= self.similarity_threshold:
                duplicate_info = {
                    'type': 'similar_content',
                    'similarity': similarity,
                    'content': post[1],
                    'topic': post[6],
                    'created_at': post[10],
                    'source': post[11]
                }
                duplicates.append(duplicate_info)
                
                # é‡è¤‡æ¤œå‡ºè¨˜éŒ²
                cursor.execute('''
                    INSERT INTO duplicate_detections 
                    (attempted_content, similar_post_id, similarity_score, detection_reason)
                    VALUES (?, ?, ?, ?)
                ''', (content, post[0], similarity, 'similar_content'))
        
        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        conn.commit()
        conn.close()
        
        return len(duplicates) > 0, duplicates
    
    def save_approved_post(self, content: str, animal_type: str = None, 
                          topic: str = None, post_type: str = None) -> bool:
        """æ‰¿èªã•ã‚ŒãŸæŠ•ç¨¿ã‚’ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            content_hash = self.calculate_content_hash(content)
            normalized_content = self.normalize_content(content)
            keywords = json.dumps(self.extract_keywords(content), ensure_ascii=False)
            main_points = json.dumps(self.extract_main_points(content), ensure_ascii=False)
            
            if not topic:
                topic = self.extract_topic(content)
            
            if not animal_type:
                animal_type = "cat" if '#çŒ«ã®ã‚ã‚Œã“ã‚Œ' in content else "dog"
            
            cursor.execute('''
                INSERT INTO post_history 
                (content, content_hash, normalized_content, post_type, animal_type, 
                 topic, keywords, main_points, char_count, source)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (content, content_hash, normalized_content, post_type, animal_type,
                  topic, keywords, main_points, len(content), 'generated'))
            
            conn.commit()
            conn.close()
            return True
            
        except Exception as e:
            print(f"âŒ æŠ•ç¨¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç·æŠ•ç¨¿æ•°
        cursor.execute('SELECT COUNT(*) FROM post_history')
        total_posts = cursor.fetchone()[0]
        
        # å‹•ç‰©ç¨®åˆ¥
        cursor.execute('SELECT animal_type, COUNT(*) FROM post_history GROUP BY animal_type')
        animal_counts = dict(cursor.fetchall())
        
        # é‡è¤‡æ¤œå‡ºæ•°
        cursor.execute('SELECT COUNT(*) FROM duplicate_detections')
        duplicate_detections = cursor.fetchone()[0]
        
        # æœ€è¿‘ã®æŠ•ç¨¿æ•°ï¼ˆ30æ—¥é–“ï¼‰
        cursor.execute('''
            SELECT COUNT(*) FROM post_history 
            WHERE created_at > datetime('now', '-30 days')
        ''')
        recent_posts = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_posts': total_posts,
            'animal_counts': animal_counts,
            'duplicate_detections': duplicate_detections,
            'recent_posts': recent_posts
        }
    
    def clean_old_posts(self, days_to_keep: int = 180):
        """å¤ã„æŠ•ç¨¿ã‚’å‰Šé™¤"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            DELETE FROM post_history 
            WHERE created_at < datetime('now', '-{} days')
            AND source = 'generated'
        '''.format(days_to_keep))
        
        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        return deleted_count